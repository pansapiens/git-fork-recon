# LLM Model configuration
MODEL=z-ai/glm-4.5-air:free
# MODEL=gemma3:27b-it-qat

# GitHub API configuration
GITHUB_TOKEN=your_github_token_here

# OpenAI-compatible API configuration
OPENAI_API_BASE_URL=https://openrouter.ai/api/v1
# OPENAI_API_BASE_URL=http://localhost:11434/v1  # Local Ollama
# OPENAI_BASE_URL=http://localhost:8080/api     # OpenWebUI

# API key configuration (use OPENAI_API_KEY or OPENROUTER_API_KEY)
# OPENROUTER_API_KEY=your_openrouter_key_here
# OPENAI_API_KEY=your_api_key_here

# Cache directory configuration
# REPO_CACHE_DIR=$HOME/.cache/git-fork-recon/repos    # Repository cache (uses platformdirs default if not set)
# REPORT_CACHE_DIR=$HOME/.cache/git-fork-recon/reports  # Report cache (server mode only, uses platformdirs default if not set)

# Optional: Override default context length
# CONTEXT_LENGTH=64000

# =============================================================================
# Server Configuration (for git-fork-recon-server)
# =============================================================================

# Comma-separated list of allowed LLM models (default: unrestricted)
# ALLOWED_MODELS=deepseek/deepseek-chat-v3.1-terminus,z-ai/glm-4.5-air

# Server host and port configuration
# SERVER_HOST=127.0.0.1
# SERVER_PORT=8000

# Authentication settings
# DISABLE_AUTH=0
# AUTH_BEARER_TOKEN=your-secret-token-here

# Maximum concurrent analysis tasks (default: 2)
# PARALLEL_TASKS=2
